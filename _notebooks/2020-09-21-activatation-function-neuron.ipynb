{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function options for a single neuron\n",
    "> Coding of activation functions commonly used in deep learning to regulate the output of basic procesing unit (neuron). \n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- hide_binder_badge: true\n",
    "- hide_colab_badge: true\n",
    "- comments: true\n",
    "- categories: [deeplearning, Julia 1.5]\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- author: Omer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, I am going to discuss how to implement the activation function of a neuron using a simple python code. A neuron is the basic processing unit of any deep learning architecture. It receives two weighted inputs (x and b), adds them together, and outputs the value (y). This value is then subjected to different activation functions (a).\n",
    "\n",
    "As the name implies, activation function allows the neuron's output to propagate to the next stage (another neuron) by mapping y to a. The different reasons to have this *activated output*; \n",
    "- to keep it in a specific range [low high]\n",
    "- to keep it positive\n",
    "- to avoid having larger values\n",
    "\n",
    "Over the years, researchers have come up with many activation functions. However, here we will be discussing the most commonly used functions;\n",
    "1. __Sigmoid__\n",
    "2. __Tanh__\n",
    "3. __RelU__\n",
    "4. __Softmax__\n",
    "\n",
    "Top three function are used in intermediate layer neurons (except for input and output layers). __Softmax__ is usually employed in the output layer. \n",
    "\n",
    "We mathematically define our activation function as \n",
    "\n",
    "\\begin{equation*}\n",
    "Z = \\Theta(y)\n",
    "\\end{equation*} \n",
    "\n",
    "where,\n",
    "$\\Theta(y)$ represents the chosen activation functions and \n",
    "$Z$ represents the activated output that will be feed to next stage neuron\n",
    "\n",
    "In Julia we do not need to import any library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "- small changes in input lead to small changes in output (activation)\n",
    "- extreme changes in input lead to extreme changes in output (activation)\n",
    "- activated output range [0 1]\n",
    "\\begin{equation*}\n",
    "\\theta(y) = \\frac{1}{1+e^{-y}}\n",
    "\\end{equation*}\n",
    "\n",
    "A single line function in Julia can be written as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(y) = round( 1/(1+exp(-y)), digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out different values of $y$, we can see that activated output is always positive and never goes beyond 1 (upper limit). \n",
    "    \n",
    "The `$` sign in print method resolves the arguments. It is convenient way to use variables and functions inside string arguments of `println` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid with w1.x+ w0.b = y = 0.0001: 0.5\n",
      "Sigmoid with w1.x+ w0.b = y = 1000  : 1.0\n",
      "Sigmoid with w1.x+ w0.b = y = -10   : 0.0\n",
      "Sigmoid with w1.x+ w0.b = y = -100  : 0.0\n",
      "Sigmoid with w1.x+ w0.b = y = -2    : 0.119\n"
     ]
    }
   ],
   "source": [
    "println(\"Sigmoid with w1.x+ w0.b = y = 0.0001: $(sigmoid(0.00001))\") \n",
    "println(\"Sigmoid with w1.x+ w0.b = y = 1000  : $(sigmoid(10000))\")\n",
    "println(\"Sigmoid with w1.x+ w0.b = y = -10   : $(sigmoid(-10))\")\n",
    "println(\"Sigmoid with w1.x+ w0.b = y = -100  : $(sigmoid(-100))\")\n",
    "println(\"Sigmoid with w1.x+ w0.b = y = -2    : $(sigmoid(-2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "- activated output range [-1 1]\n",
    "\\begin{equation*}\n",
    "\\theta(y) = \\frac{e^{y} - e^{-y}}{e^{y} + e^{-y}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tanh (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(y) = round( (exp(y) - exp(-y))/(exp(y) + exp(-y)), digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, we can see that by choosing a tanh activation function, the activated output is in the range between [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh with w1.x+ w0.b = y = 0.0001: 0.0\n",
      "tanh with w1.x+ w0.b = y = 1000  : 1.0\n",
      "tanh with w1.x+ w0.b = y = -10   : -1.0\n",
      "tanh with w1.x+ w0.b = y = -100  : -1.0\n",
      "tanh with w1.x+ w0.b = y = -2    : -0.964\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "println(\"tanh with w1.x+ w0.b = y = 0.0001: $(tanh(0.00001))\")\n",
    "println(\"tanh with w1.x+ w0.b = y = 1000  : $(tanh(100))\")\n",
    "println(\"tanh with w1.x+ w0.b = y = -10   : $(tanh(-10))\")\n",
    "println(\"tanh with w1.x+ w0.b = y = -100  : $(tanh(-100))\")\n",
    "println(\"tanh with w1.x+ w0.b = y = -2    : $(tanh(-2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu\n",
    "Rectified linear unit is the most commonly used activation function in deep learning architectures (CNN, RNN, etc.). It is mathematically defined as shown below with the activation range of [0 z] \n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta(y) = max(0,y)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relu (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def relu(y):\n",
    "#    return max(0,y)\n",
    "relu(y) = round(max(0, y), digits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see this function simply rectifies the activated output when $y$ is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relu with w1.x+ w0.b = y = 0.0001: 0.0\n",
      "Relu with w1.x+ w0.b = y = 1000  : 100.0\n",
      "Relu with w1.x+ w0.b = y = -10   : 0.0\n",
      "Relu with w1.x+ w0.b = y = -100  : 0.0\n",
      "Relu with w1.x+ w0.b = y = -2    : 0.0\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "println(\"Relu with w1.x+ w0.b = y = 0.0001: $(relu(0.00001))\")\n",
    "println(\"Relu with w1.x+ w0.b = y = 1000  : $(relu(100))\")\n",
    "println(\"Relu with w1.x+ w0.b = y = -10   : $(relu(-10))\")\n",
    "println(\"Relu with w1.x+ w0.b = y = -100  : $(relu(-100))\")\n",
    "println(\"Relu with w1.x+ w0.b = y = -2    : $(relu(-2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Softmax \n",
    "\n",
    "As mentioned before, __softmax__ is usually employed in the output layer. As an example, if there are 3 neurons in the output layer, softmax will indicate which of the three neurons has the highest activated output. This is usually done to decide the categorical output in response to an input __X__ to our neural network. \n",
    "\n",
    "Let us first define the activation function for a single neuron $i$ as \n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta(y_{i}) = e^{y_{i}} ~~~~~~~~~~~~~~~~~~~~~~~(1)\n",
    "\\end{equation*}\n",
    "\n",
    "We then normalize the activated output of each neuron by combined activation of all the $M$ neurons.\n",
    " \n",
    "\\begin{equation*}\n",
    "\\theta(y_{i}) = \\frac {e^{y_{i}}} {\\sum_{j=1}^{M} e^{y_{j}}} ~~~~~~~~~~~~~~~~~~~~~~~(2)\n",
    "\\end{equation*}\n",
    "\n",
    "for $i=1...M$ \n",
    "\n",
    "afterwards, we simply select the neuron with the largest normalized activated output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 2 methods)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(y, win=false)\n",
    "    each_neuron = [] # declare empty array\n",
    "    \n",
    "    for i in y\n",
    "        push!(each_neuron, exp(i)) # compute exp for each individual neuron (eq-1 above)\n",
    "    end\n",
    "    \n",
    "    total = sum(each_neuron)\n",
    "    for j in eachindex(each_neuron)\n",
    "        each_neuron[j] = each_neuron[j] / total # normalizing each neuron output by total (eq-2 above) \n",
    "    end\n",
    "    \n",
    "    if win #if need wining neuron info\n",
    "        val,id = findmax( round.(each_neuron, digits=3) )\n",
    "        return id\n",
    "    else\n",
    "        return round.(each_neuron, digits=3) # element-wise operation on each element of array\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show example of 3 neurons in output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax with w1.x+ w0.b = y = [-1,1,5] : [0.002, 0.018, 0.98]\n",
      "Softmax with w1.x+ w0.b = y = [0,2,1]  : [0.09, 0.665, 0.245]\n",
      "Softmax with w1.x+ w0.b = y = [-10,1,5]: [0.0, 0.018, 0.982]\n",
      "Softmax with w1.x+ w0.b = y = [5,1,5]  : [0.495, 0.009, 0.495]\n",
      "Softmax with w1.x+ w0.b = y = [3,5,0]  : [0.118, 0.876, 0.006]\n",
      "\n",
      "\n",
      " Softmax with argmax to select the winning neuro in output \n",
      "\n",
      "Softmax with w1.x+ w0.b = y = [-1,1,5] : 3\n",
      "Softmax with w1.x+ w0.b = y = [0,2,1]  : 2\n",
      "Softmax with w1.x+ w0.b = y = [-10,1,5]: 3\n",
      "Softmax with w1.x+ w0.b = y = [5,1,5]  : 1\n",
      "Softmax with w1.x+ w0.b = y = [3,5,0]  : 2\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "println(\"Softmax with w1.x+ w0.b = y = [-1,1,5] : $(softmax([-1,1,5]))\")\n",
    "println(\"Softmax with w1.x+ w0.b = y = [0,2,1]  : $(softmax([0,2,1] ))\")\n",
    "println(\"Softmax with w1.x+ w0.b = y = [-10,1,5]: $(softmax([-10,1,5]))\")\n",
    "println(\"Softmax with w1.x+ w0.b = y = [5,1,5]  : $(softmax([5,1,5]))\")\n",
    "println(\"Softmax with w1.x+ w0.b = y = [3,5,0]  : $(softmax([3,5,0]))\")\n",
    "\n",
    "println(\"\\n\\n Softmax with argmax to select the winning neuro in output \\n\")\n",
    "\n",
    "println(\"Softmax with w1.x+ w0.b = y = [-1,1,5] : $(softmax([-1,1,5],true))\")\n",
    "println(\"Softmax with w1.x+ w0.b = y = [0,2,1]  : $(softmax([0,2,1],true ))\")\n",
    "println(\"Softmax with w1.x+ w0.b = y = [-10,1,5]: $(softmax([-10,1,5],true))\")\n",
    "println(\"Softmax with w1.x+ w0.b = y = [5,1,5]  : $(softmax([5,1,5],true))\")\n",
    "println(\"Softmax with w1.x+ w0.b = y = [3,5,0]  : $(softmax([3,5,0],true))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaPro_v1.5.1-1 1.5.1",
   "language": "julia",
   "name": "juliapro_v1.5.1-1-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

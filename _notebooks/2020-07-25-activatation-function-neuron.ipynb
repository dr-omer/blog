{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function options for a single neuron\n",
    "> Coding of activation functions commonly used in deep learning to regulate the output of basic procesing unit (neuron). \n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- hide_binder_badge: true\n",
    "- comments: true\n",
    "- categories: [deeplearning, python]\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- author: Omer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, I am going to discuss how to implement the activation function of a neuron using a simple python code. A neuron is the basic processing unit of any deep learning architecture. It receives two weighted inputs (x and b), adds them together, and outputs the value (y). This value is then subjected to different activation functions (a).\n",
    "\n",
    "As the name implies, activation function allows the neuron's output to propagate to the next stage (another neuron) by mapping y to a. The different reasons to have this *activated output*; \n",
    "- to keep it in a specific range [low high]\n",
    "- to keep it positive\n",
    "- to avoid having larger values\n",
    "\n",
    "Over the years, researchers have come up with many activation functions. However, here we will be discussing the most commonly used functions;\n",
    "1. __Sigmoid__\n",
    "2. __Tanh__\n",
    "3. __RelU__\n",
    "4. __Softmax__\n",
    "\n",
    "Top three function are used in intermediate layer neurons (except for input and output layers). __Softmax__ is usually employed in the output layer. \n",
    "\n",
    "We mathematically define our activation function as \n",
    "\n",
    "\\begin{equation*}\n",
    "Z = \\Theta(y)\n",
    "\\end{equation*} \n",
    "\n",
    "where,\n",
    "$\\Theta(y)$ represents the chosen activation functions and \n",
    "$Z$ represents the activated output that will be feed to next stage neuron\n",
    "\n",
    "To begin, we import required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "# We can either use e or exp\n",
    "from math import e    # value of e. e.g., e**y \n",
    "#from math import exp # e as function. e.g., exp(y)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000453999297625"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "1/1+(e**-10)\n",
    "1/1+(exp(-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "- small changes in input lead to small changes in output (activation)\n",
    "- extreme changes in input lead to extreme changes in output (activation)\n",
    "- activated output range [0 1]\n",
    "\\begin{equation*}\n",
    "\\theta(y) = \\frac{1}{1+e^{-y}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    return 1/(1+e**-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out different values of $y$, we can see that activated output is always positive and never goes beyond 1 (upper limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid with w1.x+ w0.b = y = 0.0001: 0.500\n",
      "Sigmoid with w1.x+ w0.b = y = 1000  : 1.000\n",
      "Sigmoid with w1.x+ w0.b = y = -10   : 0.000\n",
      "Sigmoid with w1.x+ w0.b = y = -100  : 0.000\n",
      "Sigmoid with w1.x+ w0.b = y = -2    : 0.119\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = 0.0001: {sigmoid(0.00001):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = 1000  : {sigmoid(10000):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = -10   : {sigmoid(-10):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = -100  : {sigmoid(-100):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = -2    : {sigmoid(-2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "- activated output range [-1 1]\n",
    "\\begin{equation*}\n",
    "\\theta(y) = \\frac{e^{y} - e^{-y}}{e^{y} + e^{-y}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(y):\n",
    "    return (e**y - e**-y)/(e**y + e**-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, we can see that by choosing a tanh activation function, the activated output is in the range between [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid with w1.x+ w0.b = y = 0.0001: 0.000\n",
      "Sigmoid with w1.x+ w0.b = y = 100   : 1.000\n",
      "Sigmoid with w1.x+ w0.b = y = -10   : -1.000\n",
      "Sigmoid with w1.x+ w0.b = y = -100  : -1.000\n",
      "Sigmoid with w1.x+ w0.b = y = -2    : -0.964\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = 0.0001: {tanh(0.00001):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = 100   : {tanh(100):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = -10   : {tanh(-10):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = -100  : {tanh(-100):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = -2    : {tanh(-2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu\n",
    "Rectified linear unit is the most commonly used activation function in deep learning architectures (CNN, RNN, etc.). It is mathematically defined as shown below with the activation range of [0 z] \n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta(y) = max(0,y)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(y):\n",
    "    return max(0,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see this function simply rectifies the activated output when $y$ is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid with w1.x+ w0.b = y = 0.0001: 0.000\n",
      "Sigmoid with w1.x+ w0.b = y = 100   : 100.000\n",
      "Sigmoid with w1.x+ w0.b = y = -10   : 0.000\n",
      "Sigmoid with w1.x+ w0.b = y = -100  : 0.000\n",
      "Sigmoid with w1.x+ w0.b = y = -2    : 0.000\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = 0.0001: {relu(0.00001):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = 100   : {relu(100):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = -10   : {relu(-10):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = -100  : {relu(-100):.3f}')\n",
    "print(f'Sigmoid with w1.x+ w0.b = y = -2    : {relu(-2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Softmax \n",
    "\n",
    "As mentioned before, __softmax__ is usually employed in the output layer. As an example, if there are 3 neurons in the output layer, softmax will indicate which of the three neurons has the highest activated output. This is usually done to decide the categorical output in response to an input __X__ to our neural network. \n",
    "\n",
    "Let us first define the activation function for a single neuron $i$ as \n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta(y_{i}) = e^{y_{i}} ~~~~~~~~~~~~~~~~~~~~~~~(1)\n",
    "\\end{equation*}\n",
    "\n",
    "We then normalize the activated output of each neuron by combined activation of all the $M$ neurons.\n",
    " \n",
    "\\begin{equation*}\n",
    "\\theta(y_{i}) = \\frac {e^{y_{i}}} {\\sum_{j=1}^{M} e^{y_{j}}} ~~~~~~~~~~~~~~~~~~~~~~~(2)\n",
    "\\end{equation*}\n",
    "\n",
    "for $i=1...M$ \n",
    "\n",
    "afterwards, we simply select the neuron with the largest normalized activated output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y): \n",
    "    each_neuron = [e**i for i in y ] # compute exp for each individual neuron (eq-1 above)\n",
    "    return [j/sum(each_neuron) for j in each_neuron]  # normalizing each neuron output by total (eq-2 above) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show example of 3 neurons in output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid with w1.x+ w0.b = y = [-1,1,5]  :  [0.002428258029591338, 0.017942534803329198, 0.9796292071670795]\n",
      "Sigmoid with w1.x+ w0.b = y = [0,2,1]   :  [0.09003057317038048, 0.665240955774822, 0.2447284710547977]\n",
      "Sigmoid with w1.x+ w0.b = y = [-10,1,5] :  [3.0040020689707774e-07, 0.01798620455903037, 0.9820134950407627]\n",
      "Sigmoid with w1.x+ w0.b = y = [5,1,5]   :  [0.4954626425778431, 0.009074714844313748, 0.4954626425778431]\n",
      "Sigmoid with w1.x+ w0.b = y = [3,5,0]   :  [0.11849965453500957, 0.8756005950630876, 0.0058997504019027815] \n",
      "\n",
      "\n",
      "Softmax with argmax to select the winning neuro in output\n",
      "Sigmoid with w1.x+ w0.b = y = [-1,1,5]  :  2\n",
      "Sigmoid with w1.x+ w0.b = y = [0,2,1]   :  1\n",
      "Sigmoid with w1.x+ w0.b = y = [-10,1,5] :  2\n",
      "Sigmoid with w1.x+ w0.b = y = [5,1,5]   :  0\n",
      "Sigmoid with w1.x+ w0.b = y = [3,5,0]   :  1\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "print('Sigmoid with w1.x+ w0.b = y = [-1,1,5]  : ', softmax([-1,1,5]))\n",
    "print('Sigmoid with w1.x+ w0.b = y = [0,2,1]   : ', softmax([0 ,2,1]))\n",
    "print('Sigmoid with w1.x+ w0.b = y = [-10,1,5] : ', softmax([-10,1,5]))\n",
    "print('Sigmoid with w1.x+ w0.b = y = [5,1,5]   : ', softmax([5,1,5]))\n",
    "print('Sigmoid with w1.x+ w0.b = y = [3,5,0]   : ', softmax([3,5,0]),'\\n\\n')\n",
    "\n",
    "print('Softmax with argmax to select the winning neuro in output')\n",
    "print('Sigmoid with w1.x+ w0.b = y = [-1,1,5]  : ', np.argmax(softmax([-1,1,5])))\n",
    "print('Sigmoid with w1.x+ w0.b = y = [0,2,1]   : ', np.argmax(softmax([0 ,2,1])))\n",
    "print('Sigmoid with w1.x+ w0.b = y = [-10,1,5] : ', np.argmax(softmax([-10,1,5])))\n",
    "print('Sigmoid with w1.x+ w0.b = y = [5,1,5]   : ', np.argmax(softmax([5,1,5])))\n",
    "print('Sigmoid with w1.x+ w0.b = y = [3,5,0]   : ', np.argmax(softmax([3,5,0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function options for a neuron\n",
    "> In this post I am attempting to summarize the options available to modify the output of any given neuron.\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- hide_binder_badge: true\n",
    "- comments: true\n",
    "- categories: [deeplearning, python]\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- author: Omer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "from math import e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "Activation function are defined as \n",
    "\\begin{equation*}\n",
    "a = \\sigma(z)\n",
    "\\end{equation*} \n",
    "where $\\sigma(z)$ can be \n",
    " - sigmoid\n",
    " - tanh\n",
    " - ReLu\n",
    " - Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation Function\n",
    "- Small changes in input lead to small changes in output (activation)\n",
    "- exterem changes in input lead to extrem changes in output (activation)\n",
    "- activation output range [0 1]\n",
    "\\begin{equation*}\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+e**-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid with w.x+b = z = 0.0001:  0.5000024999999999\n",
      "Sigmoid with w.x+b = z = 1000  :  1.0\n",
      "Sigmoid with w.x+b = z = -10   :  4.539786870243442e-05\n",
      "Sigmoid with w.x+b = z = -100  :  3.7200759760208555e-44\n",
      "Sigmoid with w.x+b = z = -2    :  0.11920292202211757\n"
     ]
    }
   ],
   "source": [
    "print('Sigmoid with w.x+b = z = 0.0001: ', sigmoid(0.00001))\n",
    "print('Sigmoid with w.x+b = z = 1000  : ', sigmoid(10000))\n",
    "print('Sigmoid with w.x+b = z = -10   : ', sigmoid(-10))\n",
    "print('Sigmoid with w.x+b = z = -100  : ', sigmoid(-100))\n",
    "print('Sigmoid with w.x+b = z = -2    : ', sigmoid(-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh activation function\n",
    "- acitvation output range [-1 1]\n",
    "\\begin{equation*}\n",
    "\\sigma(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (e**z - e**-z)/(e**z + e**-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanh with w.x+b = z = 0.0001:  9.999999999621023e-06\n",
      "Tanh with w.x+b = z = 100   :  1.0\n",
      "Tanh with w.x+b = z = -10   :  -0.9999999958776926\n",
      "Tanh with w.x+b = z = -100  :  -1.0\n",
      "Tanh with w.x+b = z = -2    :  -0.964027580075817\n"
     ]
    }
   ],
   "source": [
    "print('Tanh with w.x+b = z = 0.0001: ', tanh(0.00001))\n",
    "print('Tanh with w.x+b = z = 100   : ', tanh(100))\n",
    "print('Tanh with w.x+b = z = -10   : ', tanh(-10))\n",
    "print('Tanh with w.x+b = z = -100  : ', tanh(-100))\n",
    "print('Tanh with w.x+b = z = -2    : ', tanh(-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu activation function \n",
    "- activation range [0 z] \n",
    "\\begin{equation*}\n",
    "a = \\sigma(z) = max(0,z)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return max(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLu with w.x+b = z = 0.0001:  1e-05\n",
      "ReLu with w.x+b = z = 100   :  100\n",
      "ReLu with w.x+b = z = -10   :  0\n",
      "ReLu with w.x+b = z = -100  :  0\n",
      "ReLu with w.x+b = z = -2    :  0\n"
     ]
    }
   ],
   "source": [
    "print('ReLu with w.x+b = z = 0.0001: ', relu(0.00001))\n",
    "print('ReLu with w.x+b = z = 100   : ', relu(100))\n",
    "print('ReLu with w.x+b = z = -10   : ', relu(-10))\n",
    "print('ReLu with w.x+b = z = -100  : ', relu(-100))\n",
    "print('ReLu with w.x+b = z = -2    : ', relu(-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Softmax activation function\n",
    "- activation range\n",
    "\n",
    "- For single ouput neuron\n",
    "\\begin{equation*}\n",
    "\\sigma(z) = e^{z} ~~~~~~~~~~~~~~~~~~~~~~~(1)\n",
    "\\end{equation*}\n",
    "\n",
    "- Combining results over all $K$ output neurons; hence softmax (normalizing each $ith$ neuron by total $K$ neurons)\n",
    " \n",
    "\\begin{equation*}\n",
    "\\sigma(z)_{i} = \\frac {e^{z_{i}}} {\\sum_{j=1}^{K} e^{z_{j}}} ~~~~~~~~~~~~~~~~~~~~~~~(2)\n",
    "\\end{equation*}\n",
    "\n",
    "for $i=1...K$ \n",
    "\n",
    "- In the end for selecting winning neuron we appy np.argmax()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "from math import exp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z): # concise defination using list comprehension of python\n",
    "    each_neuron = [exp(i) for i in z ] # compute exp for each individual neuron (eq-1 above)\n",
    "    return [j/sum(each_neuron) for j in each_neuron]  # normalizing each neuron output by total (eq-2 above) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax with w.x+b = z = [-1,1,5]   :  [0.0024282580295913376, 0.017942534803329194, 0.9796292071670795]\n",
      "SoftMax with w.x+b = z = [0,2,1]    :  [0.09003057317038046, 0.6652409557748219, 0.24472847105479764]\n",
      "SoftMax with w.x+b = z = [-10,1,5]  :  [3.0040020689707753e-07, 0.017986204559030366, 0.9820134950407627]\n",
      "SoftMax with w.x+b = z = [5,1,5]    :  [0.4954626425778431, 0.009074714844313747, 0.4954626425778431]\n",
      "SoftMax with w.x+b = z = [3,5,0]    :  [0.11849965453500957, 0.8756005950630876, 0.005899750401902781] \n",
      "\n",
      "\n",
      "Softmax with argmax to select the winning neuro in output\n",
      "SoftMax with w.x+b = z = [-1,1,5]   :  2\n",
      "SoftMax with w.x+b = z = [0,2,1]    :  1\n",
      "SoftMax with w.x+b = z = [-10,1,5]  :  2\n",
      "SoftMax with w.x+b = z = [5,1,5]    :  0\n",
      "SoftMax with w.x+b = z = [3,5,0]    :  1\n"
     ]
    }
   ],
   "source": [
    "print('SoftMax with w.x+b = z = [-1,1,5]   : ', softmax([-1,1,5]) )\n",
    "print('SoftMax with w.x+b = z = [0,2,1]    : ', softmax([0 ,2,1]) )\n",
    "print('SoftMax with w.x+b = z = [-10,1,5]  : ', softmax([-10,1,5]))\n",
    "print('SoftMax with w.x+b = z = [5,1,5]    : ', softmax([5,1,5])  )\n",
    "print('SoftMax with w.x+b = z = [3,5,0]    : ', softmax([3,5,0])  ,'\\n\\n')\n",
    "\n",
    "print('Softmax with argmax to select the winning neuro in output')\n",
    "print('SoftMax with w.x+b = z = [-1,1,5]   : ', np.argmax(softmax([-1,1,5]) ))\n",
    "print('SoftMax with w.x+b = z = [0,2,1]    : ', np.argmax(softmax([0 ,2,1]) ))\n",
    "print('SoftMax with w.x+b = z = [-10,1,5]  : ', np.argmax(softmax([-10,1,5])))\n",
    "print('SoftMax with w.x+b = z = [5,1,5]    : ', np.argmax(softmax([5,1,5])  ))\n",
    "print('SoftMax with w.x+b = z = [3,5,0]    : ', np.argmax(softmax([3,5,0])  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax with w.x+b = z = [-1,1,5]   :  [0.00242826 0.01794253 0.97962921]\n",
      "SoftMax with w.x+b = z = [0,2,1]    :  [0.09003057 0.66524096 0.24472847]\n",
      "SoftMax with w.x+b = z = [-10,1,5]  :  [3.00400207e-07 1.79862046e-02 9.82013495e-01]\n",
      "SoftMax with w.x+b = z = [5,1,5]    :  [0.49546264 0.00907471 0.49546264]\n",
      "SoftMax with w.x+b = z = [3,5,0]    :  [0.11849965 0.8756006  0.00589975] \n",
      "\n",
      "\n",
      "Softmax with argmax to select the winning neuro in output\n",
      "SoftMax with w.x+b = z = [-1,1,5]   :  2\n",
      "SoftMax with w.x+b = z = [0,2,1]    :  1\n",
      "SoftMax with w.x+b = z = [-10,1,5]  :  2\n",
      "SoftMax with w.x+b = z = [5,1,5]    :  0\n",
      "SoftMax with w.x+b = z = [3,5,0]    :  1\n"
     ]
    }
   ],
   "source": [
    "print('SoftMax with w.x+b = z = [-1,1,5]   : ', softmax_expansive([-1,1,5]) )\n",
    "print('SoftMax with w.x+b = z = [0,2,1]    : ', softmax_expansive([0 ,2,1]) )\n",
    "print('SoftMax with w.x+b = z = [-10,1,5]  : ', softmax_expansive([-10,1,5]))\n",
    "print('SoftMax with w.x+b = z = [5,1,5]    : ', softmax_expansive([5,1,5])  )\n",
    "print('SoftMax with w.x+b = z = [3,5,0]    : ', softmax_expansive([3,5,0])  ,'\\n\\n')\n",
    "\n",
    "print('Softmax with argmax to select the winning neuro in output')\n",
    "print('SoftMax with w.x+b = z = [-1,1,5]   : ', np.argmax(softmax_expansive([-1,1,5]) ))\n",
    "print('SoftMax with w.x+b = z = [0,2,1]    : ', np.argmax(softmax_expansive([0 ,2,1]) ))\n",
    "print('SoftMax with w.x+b = z = [-10,1,5]  : ', np.argmax(softmax_expansive([-10,1,5])))\n",
    "print('SoftMax with w.x+b = z = [5,1,5]    : ', np.argmax(softmax_expansive([5,1,5])  ))\n",
    "print('SoftMax with w.x+b = z = [3,5,0]    : ', np.argmax(softmax_expansive([3,5,0])  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Activation function options for a single neuron | My blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Activation function options for a single neuron" />
<meta name="author" content="Omer" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Coding of activation functions commonly used in deep learning to regulate the output of basic procesing unit (neuron)." />
<meta property="og:description" content="Coding of activation functions commonly used in deep learning to regulate the output of basic procesing unit (neuron)." />
<link rel="canonical" href="https://dr-omer.github.io/blog/deeplearning/julia%201.5/2020/09/21/activatation-function-neuron.html" />
<meta property="og:url" content="https://dr-omer.github.io/blog/deeplearning/julia%201.5/2020/09/21/activatation-function-neuron.html" />
<meta property="og:site_name" content="My blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-21T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Omer"},"headline":"Activation function options for a single neuron","description":"Coding of activation functions commonly used in deep learning to regulate the output of basic procesing unit (neuron).","datePublished":"2020-09-21T00:00:00-05:00","dateModified":"2020-09-21T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://dr-omer.github.io/blog/deeplearning/julia%201.5/2020/09/21/activatation-function-neuron.html"},"url":"https://dr-omer.github.io/blog/deeplearning/julia%201.5/2020/09/21/activatation-function-neuron.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://dr-omer.github.io/blog/feed.xml" title="My blog" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Activation function options for a single neuron | My blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Activation function options for a single neuron" />
<meta name="author" content="Omer" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Coding of activation functions commonly used in deep learning to regulate the output of basic procesing unit (neuron)." />
<meta property="og:description" content="Coding of activation functions commonly used in deep learning to regulate the output of basic procesing unit (neuron)." />
<link rel="canonical" href="https://dr-omer.github.io/blog/deeplearning/julia%201.5/2020/09/21/activatation-function-neuron.html" />
<meta property="og:url" content="https://dr-omer.github.io/blog/deeplearning/julia%201.5/2020/09/21/activatation-function-neuron.html" />
<meta property="og:site_name" content="My blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-21T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Omer"},"headline":"Activation function options for a single neuron","description":"Coding of activation functions commonly used in deep learning to regulate the output of basic procesing unit (neuron).","datePublished":"2020-09-21T00:00:00-05:00","dateModified":"2020-09-21T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://dr-omer.github.io/blog/deeplearning/julia%201.5/2020/09/21/activatation-function-neuron.html"},"url":"https://dr-omer.github.io/blog/deeplearning/julia%201.5/2020/09/21/activatation-function-neuron.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://dr-omer.github.io/blog/feed.xml" title="My blog" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">My blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Activation function options for a single neuron</h1><p class="page-description">Coding of activation functions commonly used in deep learning to regulate the output of basic procesing unit (neuron).</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-21T00:00:00-05:00" itemprop="datePublished">
        Sep 21, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Omer</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#deeplearning">deeplearning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Julia 1.5">Julia 1.5</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/dr-omer/blog/tree/master/_notebooks/2020-09-21-activatation-function-neuron.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/dr-omer/blog/blob/master/_notebooks/2020-09-21-activatation-function-neuron.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Sigmoid">Sigmoid </a></li>
<li class="toc-entry toc-h2"><a href="#Tanh">Tanh </a></li>
<li class="toc-entry toc-h2"><a href="#ReLu">ReLu </a></li>
<li class="toc-entry toc-h2"><a href="#Softmax">Softmax </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-21-activatation-function-neuron.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this post, I am going to discuss how to implement the activation function of a neuron using a simple python code. A neuron is the basic processing unit of any deep learning architecture. It receives two weighted inputs (x and b), adds them together, and outputs the value (y). This value is then subjected to different activation functions (a).</p>
<p>As the name implies, activation function allows the neuron's output to propagate to the next stage (another neuron) by mapping y to a. The different reasons to have this <em>activated output</em>;</p>
<ul>
<li>to keep it in a specific range [low high]</li>
<li>to keep it positive</li>
<li>to avoid having larger values</li>
</ul>
<p>Over the years, researchers have come up with many activation functions. However, here we will be discussing the most commonly used functions;</p>
<ol>
<li><strong>Sigmoid</strong></li>
<li><strong>Tanh</strong></li>
<li><strong>RelU</strong></li>
<li><strong>Softmax</strong></li>
</ol>
<p>Top three function are used in intermediate layer neurons (except for input and output layers). <strong>Softmax</strong> is usually employed in the output layer.</p>
<p>We mathematically define our activation function as</p>
\begin{equation*}
Z = \Theta(y)
\end{equation*}<p></p>
<p>where,
$\Theta(y)$ represents the chosen activation functions and 
$Z$ represents the activated output that will be feed to next stage neuron</p>
<p>In Julia we do not need to import any library.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sigmoid">
<a class="anchor" href="#Sigmoid" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sigmoid<a class="anchor-link" href="#Sigmoid"> </a>
</h2>
<ul>
<li>small changes in input lead to small changes in output (activation)</li>
<li>extreme changes in input lead to extreme changes in output (activation)</li>
<li>activated output range [0 1]
\begin{equation*}
\theta(y) = \frac{1}{1+e^{-y}}
\end{equation*}</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">)),</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>sigmoid (generic function with 1 method)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Trying out different values of $y$, we can see that activated output is always positive and never goes beyond 1 (upper limit)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Sigmoid with w1.x+ w0.b = y = 0.0001: $(sigmoid(0.00001))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Sigmoid with w1.x+ w0.b = y = 1000  : $(sigmoid(10000))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Sigmoid with w1.x+ w0.b = y = -10   : $(sigmoid(-10))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Sigmoid with w1.x+ w0.b = y = -100  : $(sigmoid(-100))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Sigmoid with w1.x+ w0.b = y = -2    : $(sigmoid(-2))"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Sigmoid with w1.x+ w0.b = y = 0.0001: 0.5
Sigmoid with w1.x+ w0.b = y = 1000  : 1.0
Sigmoid with w1.x+ w0.b = y = -10   : 0.0
Sigmoid with w1.x+ w0.b = y = -100  : 0.0
Sigmoid with w1.x+ w0.b = y = -2    : 0.119
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tanh">
<a class="anchor" href="#Tanh" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tanh<a class="anchor-link" href="#Tanh"> </a>
</h2>
<ul>
<li>activated output range [-1 1]
\begin{equation*}
\theta(y) = \frac{e^{y} - e^{-y}}{e^{y} + e^{-y}}
\end{equation*}</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tanh</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span> <span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">)),</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tanh (generic function with 1 method)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here again, we can see that by choosing a tanh activation function, the activated output is in the range between [-1, 1].</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"tanh with w1.x+ w0.b = y = 0.0001: $(tanh(0.00001))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"tanh with w1.x+ w0.b = y = 1000  : $(tanh(100))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"tanh with w1.x+ w0.b = y = -10   : $(tanh(-10))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"tanh with w1.x+ w0.b = y = -100  : $(tanh(-100))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"tanh with w1.x+ w0.b = y = -2    : $(tanh(-2))"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tanh with w1.x+ w0.b = y = 0.0001: 0.0
tanh with w1.x+ w0.b = y = 1000  : 1.0
tanh with w1.x+ w0.b = y = -10   : -1.0
tanh with w1.x+ w0.b = y = -100  : -1.0
tanh with w1.x+ w0.b = y = -2    : -0.964
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ReLu">
<a class="anchor" href="#ReLu" aria-hidden="true"><span class="octicon octicon-link"></span></a>ReLu<a class="anchor-link" href="#ReLu"> </a>
</h2>
<p>Rectified linear unit is the most commonly used activation function in deep learning architectures (CNN, RNN, etc.). It is mathematically defined as shown below with the activation range of [0 z]</p>
\begin{equation*}
\theta(y) = max(0,y)
\end{equation*}
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#def relu(y):</span>
<span class="c1">#    return max(0,y)</span>
<span class="n">relu</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">digits</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>relu (generic function with 1 method)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we see this function simply rectifies the activated output when $y$ is negative</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>

<span class="n">println</span><span class="p">(</span><span class="s2">"Relu with w1.x+ w0.b = y = 0.0001: $(relu(0.00001))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Relu with w1.x+ w0.b = y = 1000  : $(relu(100))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Relu with w1.x+ w0.b = y = -10   : $(relu(-10))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Relu with w1.x+ w0.b = y = -100  : $(relu(-100))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Relu with w1.x+ w0.b = y = -2    : $(relu(-2))"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Relu with w1.x+ w0.b = y = 0.0001: 0.0
Relu with w1.x+ w0.b = y = 1000  : 100.0
Relu with w1.x+ w0.b = y = -10   : 0.0
Relu with w1.x+ w0.b = y = -100  : 0.0
Relu with w1.x+ w0.b = y = -2    : 0.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Softmax">
<a class="anchor" href="#Softmax" aria-hidden="true"><span class="octicon octicon-link"></span></a>Softmax<a class="anchor-link" href="#Softmax"> </a>
</h2>
<p>As mentioned before, <strong>softmax</strong> is usually employed in the output layer. As an example, if there are 3 neurons in the output layer, softmax will indicate which of the three neurons has the highest activated output. This is usually done to decide the categorical output in response to an input <strong>X</strong> to our neural network.</p>
<p>Let us first define the activation function for a single neuron $i$ as</p>
\begin{equation*}
\theta(y_{i}) = e^{y_{i}} ~~~~~~~~~~~~~~~~~~~~~~~(1)
\end{equation*}<p>We then normalize the activated output of each neuron by combined activation of all the $M$ neurons.</p>
\begin{equation*}
\theta(y_{i}) = \frac {e^{y_{i}}} {\sum_{j=1}^{M} e^{y_{j}}} ~~~~~~~~~~~~~~~~~~~~~~~(2)
\end{equation*}<p>for $i=1...M$</p>
<p>afterwards, we simply select the neuron with the largest normalized activated output</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">function</span> <span class="n">softmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">win</span><span class="o">=</span><span class="n">false</span><span class="p">)</span>
    <span class="n">each_neuron</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># declare empty array</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">y</span>
        <span class="n">push</span><span class="o">!(</span>each_neuron, exp<span class="o">(</span>i<span class="o">))</span> # compute exp <span class="k">for</span> each individual neuron <span class="o">(</span>eq-1 above<span class="o">)</span>
    <span class="n">end</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">eachindex</span><span class="p">(</span><span class="n">each_neuron</span><span class="p">)</span>
        <span class="n">each_neuron</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">each_neuron</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">each_neuron</span><span class="p">)</span> <span class="c1"># normalizing each neuron output by total (eq-2 above) </span>
    <span class="n">end</span>
    <span class="k">if</span> <span class="n">win</span> <span class="c1">#if need wining neuron info</span>
        <span class="n">val</span><span class="p">,</span><span class="nb">id</span> <span class="o">=</span> <span class="n">findmax</span><span class="p">(</span> <span class="nb">round</span><span class="o">.</span><span class="p">(</span><span class="n">each_neuron</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="p">)</span>
        <span class="k">return</span> <span class="nb">id</span>
    <span class="k">else</span>
        <span class="k">return</span> <span class="nb">round</span><span class="o">.</span><span class="p">(</span><span class="n">each_neuron</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># element-wise operation on each element of array</span>
    <span class="n">end</span>
<span class="n">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>3</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we show example of 3 neurons in output layer</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [-1,1,5] : $(softmax([-1,1,5]))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [0,2,1]  : $(softmax([0,2,1] ))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [-10,1,5]: $(softmax([-10,1,5]))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [5,1,5]  : $(softmax([5,1,5]))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [3,5,0]  : $(softmax([3,5,0]))"</span><span class="p">)</span>

<span class="n">println</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2"> Softmax with argmax to select the winning neuro in output </span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [-1,1,5] : $(softmax([-1,1,5],true))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [0,2,1]  : $(softmax([0,2,1],true ))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [-10,1,5]: $(softmax([-10,1,5],true))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [5,1,5]  : $(softmax([5,1,5],true))"</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s2">"Softmax with w1.x+ w0.b = y = [3,5,0]  : $(softmax([3,5,0],true))"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Softmax with w1.x+ w0.b = y = [-1,1,5] : [0.002, 0.018, 1.0]
Softmax with w1.x+ w0.b = y = [0,2,1]  : [0.09, 0.725, 0.769]
Softmax with w1.x+ w0.b = y = [-10,1,5]: [0.0, 0.018, 1.0]
Softmax with w1.x+ w0.b = y = [5,1,5]  : [0.495, 0.018, 0.997]
Softmax with w1.x+ w0.b = y = [3,5,0]  : [0.118, 0.993, 0.474]


 Softmax with argmax to select the winning neuro in output 

Softmax with w1.x+ w0.b = y = [-1,1,5] : 3
Softmax with w1.x+ w0.b = y = [0,2,1]  : 3
Softmax with w1.x+ w0.b = y = [-10,1,5]: 3
Softmax with w1.x+ w0.b = y = [5,1,5]  : 3
Softmax with w1.x+ w0.b = y = [3,5,0]  : 2
</pre>
</div>
</div>

</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="dr-omer/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/deeplearning/julia%201.5/2020/09/21/activatation-function-neuron.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Powered by fastpages.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/dr-omer.github.io" title="dr-omer.github.io"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/SyedOmerGilani" title="SyedOmerGilani"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
